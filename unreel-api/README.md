# UnReel Python Backend

This is the Python implementation of the UnReel backend API, built with FastAPI, SQLAlchemy, and Google's Gemini AI. The application analyzes short-form videos from platforms like YouTube Shorts and Instagram Reels, providing detailed content analysis, transcripts, and AI-powered insights.

## Project Structure

```
unreel-api/
├── app/
│   ├── __init__.py
│   ├── main.py              # FastAPI app creation and router setup
│   ├── database.py          # SQLAlchemy engine and session setup
│   ├── models.py            # Database tables (User, Analysis)
│   ├── schemas.py           # Pydantic models (API data shapes)
│   ├── core/
│   │   ├── __init__.py
│   │   └── config.py        # Loads .env variables (using Pydantic)
│   ├── routers/
│   │   ├── __init__.py
│   │   ├── analysis_router.py # /analyze endpoint
│   │   └── chat_router.py     # /chat endpoint
│   └── services/
│       ├── __init__.py
│       ├── analysis_service.py # The main orchestrator
│       ├── media_service.py    # Native yt-dlp/ffmpeg
│       ├── ai_service.py       # Native Gemini SDK
│       ├── speech_service.py   # OpenAI Whisper speech-to-text
│       └── translation_service.py # Language detection and translation
├── tests/                   # Test files
│   ├── test_api.py          # API endpoint tests
│   ├── test_full_analysis.py # Full analysis workflow tests
│   ├── test_media_service.py # Media service tests
│   ├── test_speech.py       # Speech service tests
│   ├── test_translation_service.py # Translation service tests
│   └── run_all_tests.py     # Test runner for all tests
├── alembic/                 # Database migrations (generated by Alembic)
├── .env                     # Stores GEMINI_API_KEY, DATABASE_URL
├── .dockerignore            # Files to exclude from Docker build
├── Dockerfile               # Docker image definition
├── docker-compose.yml       # Docker Compose configuration
├── docker-compose.override.yml # Development overrides
├── requirements.txt         # Python dependencies
├── run.py                   # Application entry point
├── startup.sh               # Docker startup script
└── README.md                # This file

## Prerequisites

Before you begin, ensure you have the following installed:
- Python 3.10 or higher
- PostgreSQL database
- FFmpeg (for audio extraction)
- Git (for cloning the repository)

## Installation Guide

### 1. Installing FFmpeg

FFmpeg is required for audio extraction from videos. Follow the instructions for your operating system:

#### Windows

**Option 1: Using Chocolatey (Recommended)**
```powershell
choco install ffmpeg
```

**Option 2: Using Scoop**
```powershell
scoop install ffmpeg
```

**Option 3: Using Winget**
```powershell
winget install FFmpeg
```

**Option 4: Manual Installation**
1. Download FFmpeg from [https://www.gyan.dev/ffmpeg/builds/](https://www.gyan.dev/ffmpeg/builds/)
2. Extract the archive to a folder (e.g., `C:\ffmpeg`)
3. Add `C:\ffmpeg\bin` to your system PATH:
   - Press Win + R, type `sysdm.cpl`, and press Enter
   - Go to the "Advanced" tab
   - Click "Environment Variables"
   - Under "System Variables", find and select "Path", then click "Edit"
   - Click "New" and add the path to the ffmpeg bin directory (e.g., `C:\ffmpeg\bin`)
   - Click "OK" to save
4. Verify installation by opening a new PowerShell window and running:
   ```powershell
   ffmpeg -version
   ```

#### macOS

**Using Homebrew (Recommended)**
```bash
brew install ffmpeg
```

**Using MacPorts**
```bash
sudo port install ffmpeg
```

**Verification:**
```bash
ffmpeg -version
```

#### Linux (Ubuntu/Debian)

```bash
sudo apt update
sudo apt install ffmpeg
```

**Verification:**
```bash
ffmpeg -version
```

#### Linux (CentOS/RHEL/Fedora)

```bash
# For CentOS/RHEL
sudo yum install epel-release
sudo yum install ffmpeg

# For Fedora
sudo dnf install ffmpeg
```

**Verification:**
```bash
ffmpeg -version
```

### 2. Setting up the Python Environment

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd unreel-api
   ```

2. **Create a virtual environment:**
   ```bash
   python -m venv unreel-env
   ```

3. **Activate the virtual environment:**
   - On Windows:
     ```powershell
     .\unreel-env\Scripts\Activate.ps1
     ```
   - On macOS/Linux:
     ```bash
     source unreel-env/bin/activate
     ```

4. **Install Python dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

### 3. Obtaining a Google Gemini API Key

1. Go to [Google AI Studio](https://aistudio.google.com/)
2. Sign in with your Google account
3. Click on "Get API key" in the left sidebar
4. Select your project (or create a new one)
5. Click "Create API key"
6. Copy the generated API key and save it for the next step

**Detailed Steps with Screenshots:**
1. Navigate to https://aistudio.google.com/
2. Click "Get API key" from the left navigation panel
3. If you don't have a project, click "Create Project" and follow the prompts
4. Click "Create API key"
5. Copy the API key from the dialog that appears
6. Store this key securely as you'll need it for the environment configuration

### 4. Configuring Environment Variables

Create a `.env` file in the project root directory with the following content:

```env
# Google Gemini API Key (required)
GEMINI_API_KEY=your_actual_gemini_api_key_here

# Database configuration
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/unreel

# Server configuration
HOST=0.0.0.0
PORT=3000
```

Replace `your_actual_gemini_api_key_here` with your actual API key from Google AI Studio.

### 5. Setting up the Database

1. **Install PostgreSQL:**
   - On Windows: Download from [https://www.postgresql.org/download/windows/](https://www.postgresql.org/download/windows/)
   - On macOS: `brew install postgresql`
   - On Ubuntu/Debian: `sudo apt install postgresql postgresql-contrib`

2. **Start PostgreSQL service:**
   - On Windows: Start the PostgreSQL service from Services
   - On macOS: `brew services start postgresql`
   - On Linux: `sudo systemctl start postgresql`

3. **Create the database:**
   ```bash
   psql -U postgres
   CREATE DATABASE unreel;
   \q
   ```

## Running the Application

### Option 1: Using Docker (Recommended)

1. **Set up environment variables:**
   Create a `.env` file in the project root with your Gemini API key:
   ```bash
   GEMINI_API_KEY=your_actual_gemini_api_key_here
   ```

2. **Build and run with Docker Compose:**
   ```bash
   docker-compose up --build
   ```

3. **Access the API:**
   - API: http://localhost:3000
   - Docs: http://localhost:3000/docs

### Option 2: Local Development

1. **Ensure all prerequisites are installed** (Python, FFmpeg, PostgreSQL)

2. **Activate the virtual environment:**
   ```bash
   # On Windows
   .\unreel-env\Scripts\Activate.ps1
   
   # On macOS/Linux
   source unreel-env/bin/activate
   ```

3. **Run the application:**
   ```bash
   python run.py
   ```

4. **Access the API:**
   - API: http://localhost:3000
   - Docs: http://localhost:3000/docs

## Running Tests

The project includes several test files in the `tests/` directory. To run the tests:

1. **Activate the virtual environment:**
   ```bash
   # On Windows
   .\unreel-env\Scripts\Activate.ps1
   
   # On macOS/Linux
   source unreel-env/bin/activate
   ```

2. **Run the test runner (recommended):**
   ```bash
   # Run all tests and get a summary
   python tests/run_all_tests.py
   ```

3. **Run individual test files:**
   ```bash
   # Test the API endpoints
   python tests/test_api.py
   
   # Test the full analysis workflow
   python tests/test_full_analysis.py
   
   # Test the media service
   python tests/test_media_service.py
   
   # Test the speech service
   python tests/test_speech.py
   
   # Test the translation service
   python tests/test_translation_service.py
   ```

**Note:** Some tests may require the application to be running, especially those that test API endpoints.

## API Endpoints

### POST /api/v1/analyze
Analyze a video from a URL.

**Request:**
```json
{
  "url": "https://youtube.com/shorts/..."
}
```

**Response:**
```json
{
  "analysisId": "string",
  "originalUrl": "string",
  "status": "string",
  "metadata": {
    "title": "string",
    "uploader": "string",
    "caption": "string"
  },
  "content": {
    "summary": "string",
    "translation": "string",
    "keyTopics": ["string"],
    "mentionedResources": [
      {
        "type": "string",
        "name": "string"
      }
    ]
  },
  "fullTranscript": "string",
  "detectedLanguage": "string",
  "supportedLanguages": {
    "hi": "Hindi",
    "ta": "Tamil",
    "te": "Telugu",
    "bn": "Bengali",
    "mr": "Marathi",
    "en": "English",
    "es": "Spanish",
    "fr": "French",
    "de": "German",
    "zh": "Chinese"
  },
  "createdAt": "2025-11-16T15:30:00.000Z"
}
```

### POST /api/v1/chat
Chat with the AI about a previously analyzed video.

**Request:**
```json
{
  "analysisId": "string",
  "message": "string"
}
```

**Response:**
```json
{
  "reply": "string"
}
```

## Troubleshooting Common Issues

### 1. FFmpeg Not Found

**Error:** `ffmpeg not found, skipping audio extraction`

**Solution:**
- Ensure FFmpeg is installed correctly
- Verify FFmpeg is in your system PATH
- Restart your terminal/command prompt after installing FFmpeg
- On Windows, ensure you're using PowerShell and not Command Prompt for some installations

### 2. Database Connection Issues

**Error:** `Connection refused` or `database "unreel" does not exist`

**Solution:**
- Ensure PostgreSQL is running
- Verify the DATABASE_URL in your `.env` file
- Create the database if it doesn't exist:
  ```bash
  psql -U postgres
  CREATE DATABASE unreel;
  \q
  ```
- Check PostgreSQL service status:
  - On Windows: Check Services app for PostgreSQL service
  - On macOS: `brew services list | grep postgresql`
  - On Linux: `sudo systemctl status postgresql`

### 3. Gemini API Key Issues

**Error:** `403 Forbidden` or `API key not valid`

**Solution:**
- Verify your GEMINI_API_KEY in the `.env` file
- Ensure your API key is active in Google AI Studio
- Check that you haven't exceeded usage quotas
- Ensure there are no extra spaces or characters in the API key
- Try creating a new API key if the current one isn't working

### 4. Port Already in Use

**Error:** `OSError: [Errno 98] Address already in use`

**Solution:**
- Change the PORT in your `.env` file to a different port
- Or stop the process using the port:
  ```bash
  # On Windows
  netstat -ano | findstr :3000
  taskkill /PID <PID> /F
  
  # On macOS/Linux
  lsof -i :3000
  kill -9 <PID>
  ```

### 5. Python Dependency Issues

**Error:** `ModuleNotFoundError` or import errors

**Solution:**
- Ensure you're in the virtual environment
- Reinstall dependencies:
  ```bash
  pip install -r requirements.txt
  ```
- If issues persist, try upgrading pip first:
  ```bash
  pip install --upgrade pip
  pip install -r requirements.txt
  ```

### 6. Whisper Model Download Issues

**Issue:** Long download times or connection errors when using Whisper

**Solution:**
- The first time you run the application, Whisper will download its model (~100MB)
- Ensure you have a stable internet connection
- The download will only happen once, and the model will be cached
- If download fails, try running the application again
- Check available disk space (at least 1GB recommended)

### 7. Docker Issues

**Issue:** Docker build fails or containers won't start

**Solution:**
- Ensure Docker Desktop is running
- Check Docker has sufficient resources (memory, CPU)
- Try building with `--no-cache` flag:
  ```bash
  docker-compose build --no-cache
  ```
- Check Docker logs for specific error messages:
  ```bash
  docker-compose logs
  ```

## Development Guidelines

### Code Structure

The application follows a clean architecture pattern:
- **Routers**: Handle HTTP requests and responses
- **Services**: Contain business logic
- **Models**: Define database schemas
- **Schemas**: Define API data structures
- **Core**: Configuration and utilities

### Adding New Features

1. Create new routers in `app/routers/`
2. Implement business logic in `app/services/`
3. Add database models in `app/models.py` if needed
4. Define API schemas in `app/schemas.py`
5. Register new routers in `app/main.py`

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a pull request

## License

This project is licensed under the MIT License - see the LICENSE file for details.